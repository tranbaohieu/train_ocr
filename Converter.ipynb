{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tool.config import Cfg\n",
    "from tool.translate import build_model, process_input, translate\n",
    "import torch\n",
    "import onnxruntime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Cfg.load_config_from_file('/home/viethq/vocr/vietnamese_ocr_engine/lib/text_recognition/vietocr/config.yml')\n",
    "config['cnn']['pretrained']=False\n",
    "config['device'] = 'cpu'\n",
    "model, vocab = build_model(config)\n",
    "weight_path = '/home/viethq/vocr/vietnamese_ocr_engine/trained_model/text_recognition/vgg_seq2seq_fix.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weight\n",
    "model.load_state_dict(torch.load(weight_path, map_location=torch.device(config['device'])))\n",
    "model = model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export CNN part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cnn_part(img, save_path, model, max_seq_length=128, sos_token=1, eos_token=2): \n",
    "    with torch.no_grad(): \n",
    "        src = model.cnn(img)\n",
    "        torch.onnx.export(model.cnn, img, save_path, export_params=True, \n",
    "                        opset_version=12, do_constant_folding=True, verbose=True, \n",
    "                        input_names=['img'], output_names=['output'], \n",
    "                        dynamic_axes={'img': {0: 'batch', 1: 'channel', 2:'height', 3: 'width'}, \n",
    "                                        'output': {0: 'channel', 1: 'batch'}})\n",
    "    \n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(1, 3, 32, 475)\n",
    "src = convert_cnn_part(img, './weight/cnn.onnx', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export encoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoder_part(model, src, save_path): \n",
    "    encoder_outputs, hidden = model.transformer.encoder(src) \n",
    "    torch.onnx.export(model.transformer.encoder, src, save_path, export_params=True, \n",
    "                    opset_version=11, do_constant_folding=True, input_names=['src'], \n",
    "                    output_names=['encoder_outputs', 'hidden'], \n",
    "                    dynamic_axes={'src':{0: \"channel_input\", 1:\"batch\"}, \n",
    "                                    'encoder_outputs': {0: 'channel_output', 1:'batch'},\n",
    "                                    'hidden': {0: 'batch'}}) \n",
    "    return hidden, encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden, encoder_outputs = convert_encoder_part(model, src, './weight/encoder.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_decoder_part(model, tgt, hidden, encoder_outputs, save_path):\n",
    "    tgt = tgt[-1]\n",
    "    \n",
    "    torch.onnx.export(model.transformer.decoder,\n",
    "        (tgt, hidden, encoder_outputs),\n",
    "        save_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['tgt', 'hidden', 'encoder_outputs'],\n",
    "        output_names=['output', 'hidden_out', 'last'],\n",
    "        dynamic_axes={'tgt': {0:'batch'},\n",
    "                    'encoder_outputs':{0: \"channel_input\", 1:'batch'},\n",
    "                    'hidden': {0: 'batch'},\n",
    "                    'output': {0:'batch'},\n",
    "                    'hidden_out': {0 : 'batch'},\n",
    "                    'last': {0: 'batch'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tgt = torch.CharTensor([[1] * len(img)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_decoder_part(model, tgt, hidden, encoder_outputs, './weight/decoder.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = onnx.load('./weight/cnn.onnx')\n",
    "decoder = onnx.load('./weight/encoder.onnx')\n",
    "# encoder = onnx.load('./weight/decoder.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm model has valid schema\n",
    "onnx.checker.check_model(cnn)\n",
    "onnx.checker.check_model(decoder)\n",
    "# onnx.checker.check_model(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"graph torch_jit (\\n  %src[FLOAT, channel_inputxbatchx256]\\n) initializers (\\n  %fc.weight[FLOAT, 256x512]\\n  %fc.bias[FLOAT, 256]\\n  %onnx::Concat_149[INT64, 1]\\n  %onnx::Concat_150[INT64, 1]\\n  %onnx::GRU_191[FLOAT, 2x1536]\\n  %onnx::GRU_192[FLOAT, 2x768x256]\\n  %onnx::GRU_193[FLOAT, 2x768x256]\\n) {\\n  %onnx::Gather_11 = Shape(%src)\\n  %onnx::Gather_12 = Constant[value = <Scalar Tensor []>]()\\n  %onnx::Unsqueeze_13 = Gather[axis = 0](%onnx::Gather_11, %onnx::Gather_12)\\n  %onnx::Concat_17 = Unsqueeze[axes = [0]](%onnx::Unsqueeze_13)\\n  %onnx::ConstantOfShape_19 = Concat[axis = 0](%onnx::Concat_149, %onnx::Concat_17, %onnx::Concat_150)\\n  %hidden.1 = ConstantOfShape[value = <Tensor>](%onnx::ConstantOfShape_19)\\n  %onnx::Transpose_137, %onnx::Gather_138 = GRU[direction = 'bidirectional', hidden_size = 256, linear_before_reset = 1](%src, %onnx::GRU_192, %onnx::GRU_193, %onnx::GRU_191, %, %hidden.1)\\n  %onnx::Reshape_139 = Transpose[perm = [0, 2, 1, 3]](%onnx::Transpose_137)\\n  %onnx::Reshape_140 = Constant[value = <Tensor>]()\\n  %encoder_outputs = Reshape(%onnx::Reshape_139, %onnx::Reshape_140)\\n  %onnx::Gather_142 = Constant[value = <Scalar Tensor []>]()\\n  %onnx::Concat_143 = Gather[axis = 0](%onnx::Gather_138, %onnx::Gather_142)\\n  %onnx::Gather_144 = Constant[value = <Scalar Tensor []>]()\\n  %onnx::Concat_145 = Gather[axis = 0](%onnx::Gather_138, %onnx::Gather_144)\\n  %onnx::Gemm_146 = Concat[axis = 1](%onnx::Concat_143, %onnx::Concat_145)\\n  %onnx::Tanh_147 = Gemm[alpha = 1, beta = 1, transB = 1](%onnx::Gemm_146, %fc.weight, %fc.bias)\\n  %hidden = Tanh(%onnx::Tanh_147)\\n  return %encoder_outputs, %hidden\\n}\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Print a human readable representation of the graph\n",
    "onnx.helper.printable_graph(decoder.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/home/viethq/Downloads/test/3.jpg')\n",
    "img = process_input(img, config['dataset']['image_height'], \n",
    "                config['dataset']['image_min_width'], config['dataset']['image_max_width'])  \n",
    "img = img.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = translate(img, model)[0].tolist()\n",
    "s = vocab.decode(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with ONNX Runtime's Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inference session\n",
    "cnn_session = onnxruntime.InferenceSession(\"./weight/cnn.onnx\")\n",
    "encoder_session = onnxruntime.InferenceSession(\"./weight/encoder.onnx\")\n",
    "decoder_session = onnxruntime.InferenceSession(\"./weight/decoder.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_onnx(img, session, max_seq_length=128, sos_token=1, eos_token=2):\n",
    "    \"\"\"data: BxCxHxW\"\"\"\n",
    "    cnn_session, encoder_session, decoder_session = session\n",
    "    \n",
    "    # create cnn input\n",
    "    cnn_input = {cnn_session.get_inputs()[0].name: img}\n",
    "    src = cnn_session.run(None, cnn_input)\n",
    "    \n",
    "    # create encoder input\n",
    "    encoder_input = {encoder_session.get_inputs()[0].name: src[0]}\n",
    "    encoder_outputs, hidden = encoder_session.run(None, encoder_input)\n",
    "    translated_sentence = [[sos_token] * len(img)]\n",
    "    max_length = 0\n",
    "\n",
    "    while max_length <= max_seq_length and not all(\n",
    "        np.any(np.asarray(translated_sentence).T == eos_token, axis=1)\n",
    "    ):\n",
    "        tgt_inp = translated_sentence\n",
    "        decoder_input = {decoder_session.get_inputs()[0].name: tgt_inp[-1], decoder_session.get_inputs()[1].name: hidden, decoder_session.get_inputs()[2].name: encoder_outputs}\n",
    "\n",
    "        output, hidden, _ = decoder_session.run(None, decoder_input)\n",
    "        output = np.expand_dims(output, axis=1)\n",
    "        output = torch.Tensor(output)\n",
    "\n",
    "        values, indices = torch.topk(output, 1)\n",
    "        indices = indices[:, -1, 0]\n",
    "        indices = indices.tolist()\n",
    "\n",
    "        translated_sentence.append(indices)\n",
    "        max_length += 1\n",
    "\n",
    "        del output\n",
    "\n",
    "    translated_sentence = np.asarray(translated_sentence).T\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = (cnn_session, encoder_session, decoder_session)\n",
    "s = translate_onnx(np.array(img), session)[0].tolist()\n",
    "s = vocab.decode(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.config.settings import SIMPLE_MODEL_PATH, USE_GPU\n",
    "from lib.text_recognition_v2.aster_pytorch.demo import create_model, batch_prediction, prediction\n",
    "import os\n",
    "import torch\n",
    "class TextRecognitor:\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TextRecognitor,self).__init__()\n",
    "        self.text_recogition = create_model(resume=SIMPLE_MODEL_PATH, decoder_sdim=50, attDim=64, use_cuda=USE_GPU)\n",
    "    def recognition(self, output_after_text_detect, debug=False, log_dir=None):\n",
    "        line_list, _, _, list_ids = output_after_text_detect\n",
    "        kq, sentences, text_blocks = batch_prediction(line_list=line_list, model=self.text_recogition, list_ids=list_ids, batch_size=8)\n",
    "        return kq, sentences, text_blocks\n",
    "\n",
    "def onnx_model(model, data_input, model_name=\"text_recognizer_faster.onnx\", logs_dir = \"./trained_model/text_recognition\"):\n",
    "    # Export the model\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    torch.onnx.export(model,               # model being run\n",
    "                    data_input, # model input (or a tuple for multiple inputs)\n",
    "                    os.path.join(logs_dir, model_name),   # where to save the model (can be a file or file-like object)\n",
    "                    export_params=True,        # store the trained parameter weights inside the model file\n",
    "                    opset_version=11,          # the ONNX version to export the model to\n",
    "                    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                    input_names = ['img'],   # the model's input names\n",
    "                    output_names = ['outputs', 'probs'], # the model's output names\n",
    "                    dynamic_axes={'img': {0: 'batch', 1: 'channel', 2:'height', 3: 'width'}, \n",
    "                                    'outputs' : {0 : 'batch_size', 1: 'sequence_len'},\n",
    "                                    'probs' : {0 : 'batch_size', 1: 'sequence_len'},\n",
    "                                    \n",
    "                                }\n",
    "                    )\n",
    "if __name__ == '__main__':\n",
    "    model = create_model(resume=SIMPLE_MODEL_PATH, decoder_sdim=50, attDim=64, use_cuda=USE_GPU)\n",
    "    data_input = torch.rand(1, 3, 32, 475)\n",
    "    onnx_model(model, data_input)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00eaab38050b41bcd9f2e42979717d780d7854513c1c35aec9bf529f21b0e5f7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('ocr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
